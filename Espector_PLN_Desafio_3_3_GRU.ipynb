{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nataliaespector/CEIA_TPs_Procesamiento_Lenguaje_Natural_1/blob/main/Espector_PLN_Desafio_3_3_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3yeJGnCYxuF"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Modelo de lenguaje con tokenización por caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv5PEwGzZA9-"
      },
      "source": [
        "### Consigna\n",
        "- Seleccionar un corpus de texto sobre el cual entrenar el modelo de lenguaje.\n",
        "- Realizar el pre-procesamiento adecuado para tokenizar el corpus, estructurar el dataset y separar entre datos de entrenamiento y validación.\n",
        "- Proponer arquitecturas de redes neuronales basadas en unidades recurrentes para implementar un modelo de lenguaje.\n",
        "- Con el o los modelos que consideren adecuados, generar nuevas secuencias a partir de secuencias de contexto con las estrategias de greedy search y beam search determístico y estocástico. En este último caso observar el efecto de la temperatura en la generación de secuencias.\n",
        "\n",
        "\n",
        "### Sugerencias\n",
        "- Durante el entrenamiento, guiarse por el descenso de la perplejidad en los datos de validación para finalizar el entrenamiento. Para ello se provee un callback.\n",
        "- Explorar utilizar SimpleRNN (celda de Elman), LSTM y GRU.\n",
        "- rmsprop es el optimizador recomendado para la buena convergencia. No obstante se pueden explorar otros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y-QdFbHZYj7C"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import io\n",
        "import pickle\n",
        "\n",
        "import os\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from scipy.special import softmax\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTvXlEKQZdqx"
      },
      "source": [
        "### Datos\n",
        "Utilizaremos como dataset el libro \"Cinco Semanas en Globo\" de Julio Verne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7amy6uUaBLVD"
      },
      "outputs": [],
      "source": [
        "# descargar de textos.info\n",
        "import urllib.request\n",
        "\n",
        "# Para leer y parsear el texto en HTML de wikipedia\n",
        "import bs4 as bs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6v_ickFwBJTy"
      },
      "outputs": [],
      "source": [
        "raw_html = urllib.request.urlopen('https://www.textos.info/julio-verne/cinco-semanas-en-globo/ebook')\n",
        "raw_html = raw_html.read()\n",
        "\n",
        "# Parsear artículo, 'lxml' es el parser a utilizar\n",
        "article_html = bs.BeautifulSoup(raw_html, 'lxml')\n",
        "\n",
        "# Encontrar todos los párrafos del HTML (bajo el tag <p>)\n",
        "# y tenerlos disponible como lista\n",
        "article_paragraphs = article_html.find_all('p')\n",
        "\n",
        "article_text = ''\n",
        "\n",
        "for para in article_paragraphs:\n",
        "    article_text += para.text + ' '\n",
        "\n",
        "# pasar todo el texto a minúscula\n",
        "article_text = article_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "WBE0sSYuB-E6",
        "outputId": "69abff78-6717-4ae8-8e3b-894b71ea4c0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' el final de un discurso muy\\r\\naplaudido. —\\r\\npresentación del doctor samuel fergusson. —\\r\\n\" excelsior. \" — retrato de cuerpo entero del doctor. —\\r\\nun fatalista convencido. — comida en el traveller\\'s club. —\\r\\nnumerosos brindis de circunstancias el día 14 de enero de 1862 había asistido un numeroso auditorio\\r\\na la sesión de la real sociedad geográfica de londres, plaza de\\r\\nwaterloo, 3. el presidente, sir francis m … . comunicaba a sus\\r\\nilustres colegas un hecho importante en un discurso frecuentemente\\r\\ninterrumpido por los aplausos. aquella notable muestra de elocuencia finalizaba con unas\\r\\ncuantas frases rimbombantes en las que el patriotismo manaba a\\r\\nborbotones: \"inglaterra ha marchado siempre a la cabeza de las\\r\\nnaciones (ya se sabe que las naciones marchan universalmente a la\\r\\ncabeza unas de otras) por la intrepidez con que sus viajeros\\r\\nacometen descubrimientos geográficos. (numerosas muestras de\\r\\naprobación.) el doctor samuel fergusson, uno de sus gloriosos\\r\\nhijos, no faltará a su '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# en article text se encuentra el texto de todo el libro\n",
        "article_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP1JdiOIKQWi"
      },
      "source": [
        "### Elegir el tamaño del contexto\n",
        "\n",
        "En este caso, como el modelo de lenguaje es por caracteres, todo un gran corpus\n",
        "de texto puede ser considerado un documento en sí mismo y el tamaño de contexto\n",
        "puede ser elegido con más libertad en comparación a un modelo de lenguaje tokenizado por palabras y dividido en documentos más acotados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wumBNwdjJM3j"
      },
      "outputs": [],
      "source": [
        "# seleccionamos el tamaño de contexto\n",
        "max_context_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m5FeTaGvbDbw"
      },
      "outputs": [],
      "source": [
        "# Usaremos las utilidades de procesamiento de textos y secuencias de Keras\n",
        "from tensorflow.keras.utils import pad_sequences # se utilizará para padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "573Cg5n7VhWw"
      },
      "outputs": [],
      "source": [
        "# en este caso el vocabulario es el conjunto único de caracteres que existe en todo el texto\n",
        "chars_vocab = set(article_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwTK6xgLJd8q",
        "outputId": "80d81cdc-2889-4d26-ad60-7cba5c91a975"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# la longitud de vocabulario de caracteres es:\n",
        "len(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2W0AeQjXV1Ou"
      },
      "outputs": [],
      "source": [
        "# Construimos los dicionarios que asignan índices a caracteres y viceversa.\n",
        "# El diccionario `char2idx` servirá como tokenizador.\n",
        "char2idx = {k: v for v,k in enumerate(chars_vocab)}\n",
        "idx2char = {v: k for k,v in char2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wfy_LZaBwfbU",
        "outputId": "d3ac9863-305a-4a4d-f613-84b868265b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'e', 'u', 'r', 'f', 's', 'q', 'ü', 'a', '.', '…', '\\t', '”', 'é', ';', 'c', '6', ')', 'í', 'd', 'p', '\"', 'b', ',', 'ú', 'è', 'o', '7', 'y', '0', '?', '»', '2', '9', 'l', \"'\", '4', '~', 't', '’', 'j', 'k', '3', 'v', '-', 'i', ':', 'á', '«', 'ñ', 'ó', 'w', '¿', 'g', 'n', '1', '(', '—', 'z', '¡', '\\n', '/', 'h', ' ', 'm', '\\r', '8', '5', '!', 'x', 'º'}\n"
          ]
        }
      ],
      "source": [
        "print(chars_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Bkwb21wo8l",
        "outputId": "536ce5ce-95c5-40f4-d35f-0be0c662196b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'e', 1: 'u', 2: 'r', 3: 'f', 4: 's', 5: 'q', 6: 'ü', 7: 'a', 8: '.', 9: '…', 10: '\\t', 11: '”', 12: 'é', 13: ';', 14: 'c', 15: '6', 16: ')', 17: 'í', 18: 'd', 19: 'p', 20: '\"', 21: 'b', 22: ',', 23: 'ú', 24: 'è', 25: 'o', 26: '7', 27: 'y', 28: '0', 29: '?', 30: '»', 31: '2', 32: '9', 33: 'l', 34: \"'\", 35: '4', 36: '~', 37: 't', 38: '’', 39: 'j', 40: 'k', 41: '3', 42: 'v', 43: '-', 44: 'i', 45: ':', 46: 'á', 47: '«', 48: 'ñ', 49: 'ó', 50: 'w', 51: '¿', 52: 'g', 53: 'n', 54: '1', 55: '(', 56: '—', 57: 'z', 58: '¡', 59: '\\n', 60: '/', 61: 'h', 62: ' ', 63: 'm', 64: '\\r', 65: '8', 66: '5', 67: '!', 68: 'x', 69: 'º'}\n"
          ]
        }
      ],
      "source": [
        "print(idx2char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oIUjVU0LB0r"
      },
      "source": [
        "###  Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h07G3srdJppo"
      },
      "outputs": [],
      "source": [
        "# tokenizamos el texto completo\n",
        "tokenized_text = [char2idx[ch] for ch in article_text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PwGVSKOiJ5bj",
        "outputId": "a9dafe8a-1d03-460e-b26c-ebbfda18c094"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 3,\n",
              " 44,\n",
              " 53,\n",
              " 7,\n",
              " 33,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 62,\n",
              " 18,\n",
              " 44,\n",
              " 4,\n",
              " 14,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 25,\n",
              " 62,\n",
              " 63,\n",
              " 1,\n",
              " 27,\n",
              " 64,\n",
              " 59,\n",
              " 7,\n",
              " 19,\n",
              " 33,\n",
              " 7,\n",
              " 1,\n",
              " 18,\n",
              " 44,\n",
              " 18,\n",
              " 25,\n",
              " 8,\n",
              " 62,\n",
              " 56,\n",
              " 64,\n",
              " 59,\n",
              " 19,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 7,\n",
              " 14,\n",
              " 44,\n",
              " 49,\n",
              " 53,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 18,\n",
              " 25,\n",
              " 14,\n",
              " 37,\n",
              " 25,\n",
              " 2,\n",
              " 62,\n",
              " 4,\n",
              " 7,\n",
              " 63,\n",
              " 1,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 3,\n",
              " 0,\n",
              " 2,\n",
              " 52,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 25,\n",
              " 53,\n",
              " 8,\n",
              " 62,\n",
              " 56,\n",
              " 64,\n",
              " 59,\n",
              " 20,\n",
              " 62,\n",
              " 0,\n",
              " 68,\n",
              " 14,\n",
              " 0,\n",
              " 33,\n",
              " 4,\n",
              " 44,\n",
              " 25,\n",
              " 2,\n",
              " 8,\n",
              " 62,\n",
              " 20,\n",
              " 62,\n",
              " 56,\n",
              " 62,\n",
              " 2,\n",
              " 0,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 37,\n",
              " 25,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 14,\n",
              " 1,\n",
              " 0,\n",
              " 2,\n",
              " 19,\n",
              " 25,\n",
              " 62,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 18,\n",
              " 25,\n",
              " 14,\n",
              " 37,\n",
              " 25,\n",
              " 2,\n",
              " 8,\n",
              " 62,\n",
              " 56,\n",
              " 64,\n",
              " 59,\n",
              " 1,\n",
              " 53,\n",
              " 62,\n",
              " 3,\n",
              " 7,\n",
              " 37,\n",
              " 7,\n",
              " 33,\n",
              " 44,\n",
              " 4,\n",
              " 37,\n",
              " 7,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 53,\n",
              " 42,\n",
              " 0,\n",
              " 53,\n",
              " 14,\n",
              " 44,\n",
              " 18,\n",
              " 25,\n",
              " 8,\n",
              " 62,\n",
              " 56,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 63,\n",
              " 44,\n",
              " 18,\n",
              " 7,\n",
              " 62,\n",
              " 0,\n",
              " 53,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 42,\n",
              " 0,\n",
              " 33,\n",
              " 33,\n",
              " 0,\n",
              " 2,\n",
              " 34,\n",
              " 4,\n",
              " 62,\n",
              " 14,\n",
              " 33,\n",
              " 1,\n",
              " 21,\n",
              " 8,\n",
              " 62,\n",
              " 56,\n",
              " 64,\n",
              " 59,\n",
              " 53,\n",
              " 1,\n",
              " 63,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 4,\n",
              " 25,\n",
              " 4,\n",
              " 62,\n",
              " 21,\n",
              " 2,\n",
              " 44,\n",
              " 53,\n",
              " 18,\n",
              " 44,\n",
              " 4,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 14,\n",
              " 44,\n",
              " 2,\n",
              " 14,\n",
              " 1,\n",
              " 53,\n",
              " 4,\n",
              " 37,\n",
              " 7,\n",
              " 53,\n",
              " 14,\n",
              " 44,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 18,\n",
              " 17,\n",
              " 7,\n",
              " 62,\n",
              " 54,\n",
              " 35,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 0,\n",
              " 53,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 54,\n",
              " 65,\n",
              " 15,\n",
              " 31,\n",
              " 62,\n",
              " 61,\n",
              " 7,\n",
              " 21,\n",
              " 17,\n",
              " 7,\n",
              " 62,\n",
              " 7,\n",
              " 4,\n",
              " 44,\n",
              " 4,\n",
              " 37,\n",
              " 44,\n",
              " 18,\n",
              " 25,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 62,\n",
              " 53,\n",
              " 1,\n",
              " 63,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 4,\n",
              " 25,\n",
              " 62,\n",
              " 7,\n",
              " 1,\n",
              " 18,\n",
              " 44,\n",
              " 37,\n",
              " 25,\n",
              " 2,\n",
              " 44,\n",
              " 25,\n",
              " 64,\n",
              " 59,\n",
              " 7,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 62,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 44,\n",
              " 49,\n",
              " 53,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 62,\n",
              " 2,\n",
              " 0,\n",
              " 7,\n",
              " 33,\n",
              " 62,\n",
              " 4,\n",
              " 25,\n",
              " 14,\n",
              " 44,\n",
              " 0,\n",
              " 18,\n",
              " 7,\n",
              " 18,\n",
              " 62,\n",
              " 52,\n",
              " 0,\n",
              " 25,\n",
              " 52,\n",
              " 2,\n",
              " 46,\n",
              " 3,\n",
              " 44,\n",
              " 14,\n",
              " 7,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 33,\n",
              " 25,\n",
              " 53,\n",
              " 18,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 22,\n",
              " 62,\n",
              " 19,\n",
              " 33,\n",
              " 7,\n",
              " 57,\n",
              " 7,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 64,\n",
              " 59,\n",
              " 50,\n",
              " 7,\n",
              " 37,\n",
              " 0,\n",
              " 2,\n",
              " 33,\n",
              " 25,\n",
              " 25,\n",
              " 22,\n",
              " 62,\n",
              " 41,\n",
              " 8,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 19,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 44,\n",
              " 18,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 22,\n",
              " 62,\n",
              " 4,\n",
              " 44,\n",
              " 2,\n",
              " 62,\n",
              " 3,\n",
              " 2,\n",
              " 7,\n",
              " 53,\n",
              " 14,\n",
              " 44,\n",
              " 4,\n",
              " 62,\n",
              " 63,\n",
              " 62,\n",
              " 9,\n",
              " 62,\n",
              " 8,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 63,\n",
              " 1,\n",
              " 53,\n",
              " 44,\n",
              " 14,\n",
              " 7,\n",
              " 21,\n",
              " 7,\n",
              " 62,\n",
              " 7,\n",
              " 62,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 64,\n",
              " 59,\n",
              " 44,\n",
              " 33,\n",
              " 1,\n",
              " 4,\n",
              " 37,\n",
              " 2,\n",
              " 0,\n",
              " 4,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 33,\n",
              " 0,\n",
              " 52,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 62,\n",
              " 61,\n",
              " 0,\n",
              " 14,\n",
              " 61,\n",
              " 25,\n",
              " 62,\n",
              " 44,\n",
              " 63,\n",
              " 19,\n",
              " 25,\n",
              " 2,\n",
              " 37,\n",
              " 7,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 62,\n",
              " 0,\n",
              " 53,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 62,\n",
              " 18,\n",
              " 44,\n",
              " 4,\n",
              " 14,\n",
              " 1,\n",
              " 2,\n",
              " 4,\n",
              " 25,\n",
              " 62,\n",
              " 3,\n",
              " 2,\n",
              " 0,\n",
              " 14,\n",
              " 1,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 63,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 64,\n",
              " 59,\n",
              " 44,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 1,\n",
              " 63,\n",
              " 19,\n",
              " 44,\n",
              " 18,\n",
              " 25,\n",
              " 62,\n",
              " 19,\n",
              " 25,\n",
              " 2,\n",
              " 62,\n",
              " 33,\n",
              " 25,\n",
              " 4,\n",
              " 62,\n",
              " 7,\n",
              " 19,\n",
              " 33,\n",
              " 7,\n",
              " 1,\n",
              " 4,\n",
              " 25,\n",
              " 4,\n",
              " 8,\n",
              " 62,\n",
              " 7,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 33,\n",
              " 33,\n",
              " 7,\n",
              " 62,\n",
              " 53,\n",
              " 25,\n",
              " 37,\n",
              " 7,\n",
              " 21,\n",
              " 33,\n",
              " 0,\n",
              " 62,\n",
              " 63,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 25,\n",
              " 14,\n",
              " 1,\n",
              " 0,\n",
              " 53,\n",
              " 14,\n",
              " 44,\n",
              " 7,\n",
              " 62,\n",
              " 3,\n",
              " 44,\n",
              " 53,\n",
              " 7,\n",
              " 33,\n",
              " 44,\n",
              " 57,\n",
              " 7,\n",
              " 21,\n",
              " 7,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 53,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 7,\n",
              " 4,\n",
              " 64,\n",
              " 59,\n",
              " 14,\n",
              " 1,\n",
              " 7,\n",
              " 53,\n",
              " 37,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 3,\n",
              " 2,\n",
              " 7,\n",
              " 4,\n",
              " 0,\n",
              " 4,\n",
              " 62,\n",
              " 2,\n",
              " 44,\n",
              " 63,\n",
              " 21,\n",
              " 25,\n",
              " 63,\n",
              " 21,\n",
              " 7,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 4,\n",
              " 62,\n",
              " 0,\n",
              " 53,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 19,\n",
              " 7,\n",
              " 37,\n",
              " 2,\n",
              " 44,\n",
              " 25,\n",
              " 37,\n",
              " 44,\n",
              " 4,\n",
              " 63,\n",
              " 25,\n",
              " 62,\n",
              " 63,\n",
              " 7,\n",
              " 53,\n",
              " 7,\n",
              " 21,\n",
              " 7,\n",
              " 62,\n",
              " 7,\n",
              " 64,\n",
              " 59,\n",
              " 21,\n",
              " 25,\n",
              " 2,\n",
              " 21,\n",
              " 25,\n",
              " 37,\n",
              " 25,\n",
              " 53,\n",
              " 0,\n",
              " 4,\n",
              " 45,\n",
              " 62,\n",
              " 20,\n",
              " 44,\n",
              " 53,\n",
              " 52,\n",
              " 33,\n",
              " 7,\n",
              " 37,\n",
              " 0,\n",
              " 2,\n",
              " 2,\n",
              " 7,\n",
              " 62,\n",
              " 61,\n",
              " 7,\n",
              " 62,\n",
              " 63,\n",
              " 7,\n",
              " 2,\n",
              " 14,\n",
              " 61,\n",
              " 7,\n",
              " 18,\n",
              " 25,\n",
              " 62,\n",
              " 4,\n",
              " 44,\n",
              " 0,\n",
              " 63,\n",
              " 19,\n",
              " 2,\n",
              " 0,\n",
              " 62,\n",
              " 7,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 62,\n",
              " 14,\n",
              " 7,\n",
              " 21,\n",
              " 0,\n",
              " 57,\n",
              " 7,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 4,\n",
              " 64,\n",
              " 59,\n",
              " 53,\n",
              " 7,\n",
              " 14,\n",
              " 44,\n",
              " 25,\n",
              " 53,\n",
              " 0,\n",
              " 4,\n",
              " 62,\n",
              " 55,\n",
              " 27,\n",
              " 7,\n",
              " 62,\n",
              " 4,\n",
              " 0,\n",
              " 62,\n",
              " 4,\n",
              " 7,\n",
              " 21,\n",
              " 0,\n",
              " 62,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 53,\n",
              " 7,\n",
              " 14,\n",
              " 44,\n",
              " 25,\n",
              " 53,\n",
              " 0,\n",
              " 4,\n",
              " 62,\n",
              " 63,\n",
              " 7,\n",
              " 2,\n",
              " 14,\n",
              " 61,\n",
              " 7,\n",
              " 53,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 44,\n",
              " 42,\n",
              " 0,\n",
              " 2,\n",
              " 4,\n",
              " 7,\n",
              " 33,\n",
              " 63,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 0,\n",
              " 62,\n",
              " 7,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 64,\n",
              " 59,\n",
              " 14,\n",
              " 7,\n",
              " 21,\n",
              " 0,\n",
              " 57,\n",
              " 7,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 25,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 4,\n",
              " 16,\n",
              " 62,\n",
              " 19,\n",
              " 25,\n",
              " 2,\n",
              " 62,\n",
              " 33,\n",
              " 7,\n",
              " 62,\n",
              " 44,\n",
              " 53,\n",
              " 37,\n",
              " 2,\n",
              " 0,\n",
              " 19,\n",
              " 44,\n",
              " 18,\n",
              " 0,\n",
              " 57,\n",
              " 62,\n",
              " 14,\n",
              " 25,\n",
              " 53,\n",
              " 62,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 62,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 62,\n",
              " 42,\n",
              " 44,\n",
              " 7,\n",
              " 39,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 4,\n",
              " 64,\n",
              " 59,\n",
              " 7,\n",
              " 14,\n",
              " 25,\n",
              " 63,\n",
              " 0,\n",
              " 37,\n",
              " 0,\n",
              " 53,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 4,\n",
              " 14,\n",
              " 1,\n",
              " 21,\n",
              " 2,\n",
              " 44,\n",
              " 63,\n",
              " 44,\n",
              " 0,\n",
              " 53,\n",
              " 37,\n",
              " 25,\n",
              " 4,\n",
              " 62,\n",
              " 52,\n",
              " 0,\n",
              " 25,\n",
              " 52,\n",
              " 2,\n",
              " 46,\n",
              " 3,\n",
              " 44,\n",
              " 14,\n",
              " 25,\n",
              " 4,\n",
              " 8,\n",
              " 62,\n",
              " 55,\n",
              " 53,\n",
              " 1,\n",
              " 63,\n",
              " 0,\n",
              " 2,\n",
              " 25,\n",
              " 4,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 63,\n",
              " 1,\n",
              " 0,\n",
              " 4,\n",
              " 37,\n",
              " 2,\n",
              " 7,\n",
              " 4,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 64,\n",
              " 59,\n",
              " 7,\n",
              " 19,\n",
              " 2,\n",
              " 25,\n",
              " 21,\n",
              " 7,\n",
              " 14,\n",
              " 44,\n",
              " 49,\n",
              " 53,\n",
              " 8,\n",
              " 16,\n",
              " 62,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 18,\n",
              " 25,\n",
              " 14,\n",
              " 37,\n",
              " 25,\n",
              " 2,\n",
              " 62,\n",
              " 4,\n",
              " 7,\n",
              " 63,\n",
              " 1,\n",
              " 0,\n",
              " 33,\n",
              " 62,\n",
              " 3,\n",
              " 0,\n",
              " 2,\n",
              " 52,\n",
              " 1,\n",
              " 4,\n",
              " 4,\n",
              " 25,\n",
              " 53,\n",
              " 22,\n",
              " 62,\n",
              " 1,\n",
              " 53,\n",
              " 25,\n",
              " 62,\n",
              " 18,\n",
              " 0,\n",
              " 62,\n",
              " 4,\n",
              " 1,\n",
              " 4,\n",
              " 62,\n",
              " 52,\n",
              " 33,\n",
              " 25,\n",
              " 2,\n",
              " 44,\n",
              " 25,\n",
              " 4,\n",
              " 25,\n",
              " 4,\n",
              " 64,\n",
              " 59,\n",
              " 61,\n",
              " 44,\n",
              " 39,\n",
              " 25,\n",
              " 4,\n",
              " 22,\n",
              " 62,\n",
              " 53,\n",
              " 25,\n",
              " 62,\n",
              " 3,\n",
              " 7,\n",
              " 33,\n",
              " 37,\n",
              " 7,\n",
              " 2,\n",
              " 46,\n",
              " 62,\n",
              " 7,\n",
              " 62,\n",
              " 4,\n",
              " 1,\n",
              " 62]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "tokenized_text[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfpYcaypKcI9"
      },
      "source": [
        "### Organizando y estructurando el dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WSSmg9jtKP0T"
      },
      "outputs": [],
      "source": [
        "# separaremos el dataset entre entrenamiento y validación.\n",
        "# `p_val` será la proporción del corpus que se reservará para validación\n",
        "# `num_val` es la cantidad de secuencias de tamaño `max_context_size` que se usará en validación\n",
        "p_val = 0.1\n",
        "num_val = int(np.ceil(len(tokenized_text)*p_val/max_context_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "b7dCpGrdKll0"
      },
      "outputs": [],
      "source": [
        "# separamos la porción de texto utilizada en entrenamiento de la de validación.\n",
        "train_text = tokenized_text[:-num_val*max_context_size]\n",
        "val_text = tokenized_text[-num_val*max_context_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NmxQdxl8LRCg"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_val = [val_text[init*max_context_size:init*(max_context_size+1)] for init in range(num_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_gyFT9koLqDm"
      },
      "outputs": [],
      "source": [
        "tokenized_sentences_train = [train_text[init:init+max_context_size] for init in range(len(train_text)-max_context_size+1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oVNqmmLRodT0"
      },
      "outputs": [],
      "source": [
        "X = np.array(tokenized_sentences_train[:-1])\n",
        "y = np.array(tokenized_sentences_train[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vken7O4ETsAJ"
      },
      "source": [
        "Nótese que estamos estructurando el problema de aprendizaje como *many-to-many*:\n",
        "\n",
        "Entrada: secuencia de tokens [$x_0$, $x_1$, ..., $x_N$]\n",
        "\n",
        "Target: secuencia de tokens [$x_1$, $x_2$, ..., $x_{N+1}$]\n",
        "\n",
        "De manera que la red tiene que aprender que su salida deben ser los tokens desplazados en una posición y un nuevo token predicho (el N+1).\n",
        "\n",
        "La ventaja de estructurar el aprendizaje de esta manera es que para cada token de target se propaga una señal de gradiente por el grafo de cómputo recurrente, que es mejor que estructurar el problema como *many-to-one* en donde sólo una señal de gradiente se propaga."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iPTx-UJl6r"
      },
      "source": [
        "En este punto tenemos en la variable `tokenized_sentences` los versos tokenizados. Vamos a quedarnos con un conjunto de validación que utilizaremos para medir la calidad de la generación de secuencias con la métrica de Perplejidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFAyA4zCWE-5",
        "outputId": "d04ea06f-7085-4fbb-d5da-513b8352cb83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(432714, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKRl70HFTzG",
        "outputId": "ad35adb5-faad-4528-d82f-435e8d7fc286"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([62,  0, 33, 62,  3, 44, 53,  7, 33, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVpLCKSZFXZO",
        "outputId": "186b1902-5ac4-42e4-b491-b8d669dd4c79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 33, 62,  3, 44, 53,  7, 33, 62, 18])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "y[0,:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wOFCR-KqbW1N"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(chars_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnnjdAQ5UAEJ"
      },
      "source": [
        "# Definiendo el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgz7VKwTUbj6"
      },
      "source": [
        "El modelo que se propone como ejemplo consume los índices de los tokens y los transforma en vectores OHE (en este caso no entrenamos una capa de embedding para caracteres). Esa transformación se logra combinando las capas `CategoryEncoding` que transforma a índices a vectores OHE y `TimeDistributed` que aplica la capa a lo largo de la dimensión \"temporal\" de la secuencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i4UGQwRaEwF",
        "outputId": "4bf26755-df60-408d-f9fd-ddc5295e47bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRUModel(\n",
            "  (gru): GRU(70, 200, batch_first=True)\n",
            "  (fc): Linear(in_features=200, out_features=70, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=200):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=vocab_size,\n",
        "            hidden_size=hidden_size,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, 1) con índices enteros\n",
        "        x = x.squeeze(-1).long()                   # (batch, seq_len)\n",
        "        x = F.one_hot(x, num_classes=self.vocab_size).float()  # (batch, seq_len, vocab_size)\n",
        "\n",
        "        out, _ = self.gru(x)                      # (batch, seq_len, hidden_size)\n",
        "        out = self.fc(out)                        # (batch, seq_len, vocab_size)\n",
        "        return out                                # logits (sin softmax)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = GRUModel(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmJWNyxQwfCE"
      },
      "source": [
        "\n",
        "### Definir el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zUHX3r5JD-MG"
      },
      "outputs": [],
      "source": [
        "class TrainerWithPerplexity:\n",
        "    def __init__(self, model, optimizer, criterion, train_loader, val_loader, patience=5, device=\"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.patience = patience\n",
        "        self.device = device\n",
        "\n",
        "        self.min_score = float(\"inf\")\n",
        "        self.patience_counter = 0\n",
        "        self.history_ppl = []\n",
        "\n",
        "    def compute_perplexity(self):\n",
        "        self.model.eval()\n",
        "        scores = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in self.val_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                logits = self.model(xb)  # (batch, seq_len, vocab_size)\n",
        "\n",
        "                # tomamos la probabilidad del último token predicho\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                target = yb[:, -1]  # último token\n",
        "                probs = log_probs[:, -1, :]\n",
        "\n",
        "                chosen_log_probs = probs[range(len(target)), target]\n",
        "                ppl = torch.exp(-chosen_log_probs.mean()).item()\n",
        "                scores.append(ppl)\n",
        "        return np.mean(scores)\n",
        "\n",
        "    def train(self, num_epochs=20, save_path=\"best_model.pt\"):\n",
        "        for epoch in range(num_epochs):\n",
        "            self.model.train()\n",
        "            total_loss = 0\n",
        "            for xb, yb in self.train_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                self.optimizer.zero_grad()\n",
        "                logits = self.model(xb)\n",
        "\n",
        "                # logits: (batch, seq_len, vocab_size)\n",
        "                # target: (batch, seq_len)\n",
        "                loss = self.criterion(logits.transpose(1, 2), yb)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / len(self.train_loader)\n",
        "\n",
        "            # calcular ppl en validación\n",
        "            current_ppl = self.compute_perplexity()\n",
        "            self.history_ppl.append(current_ppl)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}: train loss={avg_loss:.4f}, val ppl={current_ppl:.4f}\")\n",
        "\n",
        "            # early stopping\n",
        "            if current_ppl < self.min_score:\n",
        "                self.min_score = current_ppl\n",
        "                torch.save(self.model.state_dict(), save_path)\n",
        "                print(\"Saved new best model!\")\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "                if self.patience_counter >= self.patience:\n",
        "                    break\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HBZIwR0gruA"
      },
      "source": [
        "### Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFPA_e2kdWj9",
        "outputId": "aeaabb1a-552e-4bff-8e09-7d813b1584e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - loss: 2.0627 - val_ppl: 5.3614\n",
            "Saved new best model!\n",
            "Epoch 2/20 - loss: 1.5289 - val_ppl: 4.3848\n",
            "Saved new best model!\n",
            "Epoch 3/20 - loss: 1.3491 - val_ppl: 4.2027\n",
            "Saved new best model!\n",
            "Epoch 4/20 - loss: 1.2540 - val_ppl: 4.2291\n",
            "Epoch 5/20 - loss: 1.1898 - val_ppl: 4.3139\n",
            "Epoch 6/20 - loss: 1.1421 - val_ppl: 4.4558\n",
            "Epoch 7/20 - loss: 1.1052 - val_ppl: 4.6232\n",
            "Epoch 8/20 - loss: 1.0767 - val_ppl: 4.7966\n",
            "Early stopping triggered\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Parámetros\n",
        "batch_size = 256\n",
        "num_epochs = 20\n",
        "patience = 5\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X, dtype=torch.long),\n",
        "                                               torch.tensor(y, dtype=torch.long))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "def prepare_val_data(val_data, max_context_size, vocab_size):\n",
        "    targets, padded = [], []\n",
        "    info = []\n",
        "    count = 0\n",
        "\n",
        "    for seq in val_data:\n",
        "        len_seq = len(seq)\n",
        "        subseq = [seq[:i] for i in range(1, len_seq)]\n",
        "        targets.extend([seq[i] for i in range(1, len_seq)])\n",
        "\n",
        "        if len(subseq) != 0:\n",
        "            arr = np.zeros((len(subseq), max_context_size), dtype=np.int64)\n",
        "            for j, s in enumerate(subseq):\n",
        "                # truncar si es más larga\n",
        "                s = s[-max_context_size:]\n",
        "                arr[j, -len(s):] = s\n",
        "            padded.append(arr)\n",
        "            info.append((count, count + len_seq))\n",
        "            count += len_seq\n",
        "\n",
        "    padded = np.vstack(padded)\n",
        "    return torch.tensor(padded, dtype=torch.long), torch.tensor(targets, dtype=torch.long), info\n",
        "\n",
        "\n",
        "def compute_perplexity(model, val_inputs, val_targets, batch_size=256):\n",
        "    model.eval()\n",
        "    all_log_probs = []\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_inputs), batch_size):\n",
        "            xb = val_inputs[i:i+batch_size].to(device)\n",
        "            yb = val_targets[i:i+batch_size].to(device)\n",
        "\n",
        "            logits = model(xb.unsqueeze(-1))  # tu modelo espera (batch, seq_len, 1)\n",
        "            log_probs = F.log_softmax(logits[:, -1, :], dim=-1)\n",
        "            chosen = log_probs[range(len(yb)), yb]\n",
        "            all_log_probs.extend(chosen.cpu().numpy())\n",
        "\n",
        "    all_log_probs = np.array(all_log_probs)\n",
        "    ppl = float(np.exp(-all_log_probs.mean()))\n",
        "    return ppl\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "\n",
        "history_ppl = []\n",
        "min_score = float(\"inf\")\n",
        "patience_counter = 0\n",
        "\n",
        "# preparar datos de validación\n",
        "val_inputs, val_targets, val_info = prepare_val_data(tokenized_sentences_val,\n",
        "                                                     max_context_size=max_context_size,\n",
        "                                                     vocab_size=vocab_size)\n",
        "\n",
        "# --- training loop ---\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb.unsqueeze(-1))   # logits: (batch, seq_len, vocab_size)\n",
        "        loss = criterion(logits.transpose(1, 2), yb)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # <- NUEVO\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    current_ppl = compute_perplexity(model, val_inputs, val_targets, batch_size=batch_size)\n",
        "    history_ppl.append(current_ppl)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - loss: {avg_loss:.4f} - val_ppl: {current_ppl:.4f}\")\n",
        "\n",
        "    # early stopping\n",
        "    if current_ppl < min_score:\n",
        "        min_score = current_ppl\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "        print(\"Saved new best model!\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K30JHB3Dv-mx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "4b715a8b-118c-4d74-a5b5-3a6f5c900d70"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ6VJREFUeJzt3Xl4VOX5//H3zGQlZCchK0FACBICJLEISP214EqpK1hEUWprraio1S9itUpdgkttbakUsGqtIsUNN0RFRaRIycKO7Et21iSTdZLMnN8fkGiUJSHJnEzm87qu+WNOzuTcE5b55Hnu8zwWwzAMRERERExiNbsAERER8W4KIyIiImIqhRERERExlcKIiIiImEphREREREylMCIiIiKmUhgRERERUymMiIiIiKl8zC6gJVwuF0VFRQQHB2OxWMwuR0RERFrAMAwqKiqIi4vDaj35+IdHhJGioiISExPNLkNERETOQH5+PgkJCSf9eqvCyCOPPMKsWbOaHRswYADbtm077WsXLVrEpEmTuPzyy1myZElrLktwcDBw7M2EhIS06rUiIiJiDrvdTmJiYtPn+Mm0emRk0KBBLF++/Ntv4HP6b7Fv3z7uvfdeRo8e3drLATRNzYSEhCiMiIiIeJjTtVi0Ooz4+PgQExPT4vOdTieTJ09m1qxZfPXVV5SVlbX2kiIiItKFtfpump07dxIXF0efPn2YPHkyeXl5pzz/j3/8I9HR0dx8880tvobD4cButzd7iIiISNfUqjAyfPhwXn75ZZYtW8bcuXPZu3cvo0ePpqKi4oTnr1q1in/+858sWLCgVUVlZmYSGhra9FDzqoiISNdlMQzDONMXl5WVkZSUxLPPPvuDkY+KigpSU1N5/vnnufTSSwG46aabKCsrO20Dq8PhwOFwND1vbIApLy9Xz4iIiIiHsNvthIaGnvbzu0239oaFhdG/f3927dr1g6/t3r2bffv2MX78+KZjLpfr2EV9fNi+fTt9+/Y94ff19/fH39+/LaWJiIiIh2hTGKmsrGT37t3ccMMNP/hacnIymzZtanbswQcfpKKigueee05TLyIiIgK0Mozce++9jB8/nqSkJIqKinj44Yex2WxMmjQJgClTphAfH09mZiYBAQGkpKQ0e31YWBjAD46LiIiI92pVGCkoKGDSpEkcOXKEqKgozj//fNasWUNUVBQAeXl5p1zuVUREROT72tTA6i4tbYARERGRzqOln98axhARERFTKYyIiIiIqRRGRERExFReHUZe+99+7nx9HSXltWaXIiIi4rW8Ooy8vjaP9zYUkbXvqNmliIiIeC2vDiMZSREA5OwvNbkSERER7+XVYSQtKRyA3DyFEREREbN4dRhJPx5GthTZqa5rMLkaERER7+TVYSQuNICYkACcLoONBeVmlyMiIuKVvDqMWCwW0nsfGx1R34iIiIg5vDqMAKT3UhgRERExk8LId5pYXa5Ov02PiIhIl+P1YeScuBACfK2UVdez53CV2eWIiIh4Ha8PI742K0MSwgDI2a/Fz0RERNzN68MIfDtVo74RERER91MYQWFERETETAojQNrxO2p2H6qitKrO5GpERES8i8IIEB7kR9+oIEBLw4uIiLibwshxmqoRERExh8LIcQojIiIi5lAYOa4xjGwoKKPe6TK5GhEREe+hMHJcnx7dCQ30pbbexdYiu9nliIiIeA2FkeOsVoumakREREygMPIdTWFEd9SIiIi4jcLIdzSuN5KrkRERERG3URj5jiGJodisForLaykqqzG7HBEREa+gMPId3fx8GBQXAkC2RkdERETcQmHkezRVIyIi4l4KI9+jO2pERETcS2HkexrDyNZiO9V1DSZXIyIi0vUpjHxPXFggcaEBOF0G6/PLzC5HRESky1MYOYG0JPWNiIiIuIvCyAmob0RERMR9FEZOoDGM5OaV4XIZJlcjIiLStSmMnMDA2BACfW2U19Sz+1Cl2eWIiIh0aQojJ+BrszIkMRTQVI2IiEhHUxg5CfWNiIiIuIfCyEloB18RERH3UBg5icZl4fccquJoVZ3J1YiIiHRdCiMnEdbNj37R3QGtNyIiItKRFEZOIb2XpmpEREQ6msLIKaiJVUREpOMpjJxC47LwG/LLqHe6TK5GRESka1IYOYW+UUGEdfPF0eBiS5Hd7HJERES6JIWRU7BYLN/2jWiqRkREpEMojJyGdvAVERHpWAojp9HYxJq9/yiGoU3zRERE2lurwsgjjzyCxWJp9khOTj7p+QsWLGD06NGEh4cTHh7O2LFjWbt2bZuLdqchCWH4WC0csDsoLKsxuxwREZEup9UjI4MGDaK4uLjpsWrVqpOeu2LFCiZNmsQXX3zB119/TWJiIhdddBGFhYVtKtqdAv1sDIoLAdQ3IiIi0hF8Wv0CHx9iYmJadO5rr73W7PkLL7zAW2+9xWeffcaUKVNae2nTpCWFs6GgnNz9pVw+NN7sckRERLqUVo+M7Ny5k7i4OPr06cPkyZPJy8tr8Wurq6upr68nIiLilOc5HA7sdnuzh5m0aZ6IiEjHaVUYGT58OC+//DLLli1j7ty57N27l9GjR1NRUdGi18+YMYO4uDjGjh17yvMyMzMJDQ1teiQmJramzHbXGEa+Ka6gytFgai0iIiJdjcVowy0iZWVlJCUl8eyzz3LzzTef8tzZs2fz1FNPsWLFClJTU095rsPhwOFwND232+0kJiZSXl5OSEjImZbbJqNmf05hWQ0LfzWckf16mFKDiIiIJ7Hb7YSGhp7287tNt/aGhYXRv39/du3adcrznnnmGWbPns0nn3xy2iAC4O/vT0hISLOH2dK0T42IiEiHaFMYqaysZPfu3cTGxp70nKeeeopHH32UZcuWkZGR0ZbLmSq9VxigvhEREZH21qowcu+99/Lll1+yb98+Vq9ezZVXXonNZmPSpEkATJkyhZkzZzad/+STT/LQQw/x4osv0rt3b0pKSigpKaGysrJ934UbpCcda7rN3V+Ky6XFz0RERNpLq8JIQUEBkyZNYsCAAUycOJHIyEjWrFlDVFQUAHl5eRQXFzedP3fuXOrq6rjmmmuIjY1tejzzzDPt+y7cYGBsMIG+Nuy1Dew65HlhSkREpLNq1TojixYtOuXXV6xY0ez5vn37WltPp+VjszI0MYyv9xwhZ38p/XsGm12SiIhIl6C9aVohXU2sIiIi7U5hpBUURkRERNqfwkgrpPU6Fkb2Hq7iSKXjNGeLiIhISyiMtEJoN1/Oju4OQG5embnFiIiIdBEKI62kqRoREZH2pTDSSo0rseYqjIiIiLQLhZFWyjgeRjYUlFHX4DK5GhEREc+nMNJKZ/UIIrybL44GF1uKys0uR0RExOMpjLSSxWJR34iIiEg7Uhg5A019I9o0T0REpM0URs5A+vH1RrL3lWIY2jRPRESkLRRGzsCQxDB8rBYOVjgoKK0xuxwRERGPpjByBgJ8bQyKDwU0VSMiItJWCiNnqHGqRk2sIiIibaMwcoYa76jJ3qcwIiIi0hYKI2coo/exMLKtxE6lo8HkakRERDyXwsgZ6hkSQHxYIC4DNuSXmV2OiIiIx1IYaQMtfiYiItJ2CiNt0NQ3ojAiIiJyxhRG2qAxjKzbX4rLpcXPREREzoTCSBskxwTTzc9GhaOBnQcrzS5HRETEIymMtIGPzcrQxDBAfSMiIiJnSmGkjdTEKiIi0jYKI230bRg5anIlIiIinklhpI2GHV8Wft+Rag5XOkyuRkRExPMojLRRaKAv/Xt2ByBXUzUiIiKtpjDSDpqmarSDr4iISKspjLSDtMYdfLVpnoiISKspjLSDjN4RAGwsLMfR4DS5GhEREc+iMNIOekd2IyLIj7oGF1uK7GaXIyIi4lEURtqBxWJpmqpRE6uIiEjrKIy0k6ZN89Q3IiIi0ioKI+0ko/e3d9QYhjbNExERaSmFkXYyOD4UX5uFQxUOCkprzC5HRETEYyiMtJMAXxuD4kIB7VMjIiLSGgoj7aipb0T71IiIiLSYwkg7ymjaNK/M3EJEREQ8iMJIO0o7Hka2l9ipqK03uRoRERHPoDDSjnqGBJAQHojLgA355WaXIyIi4hEURtpZ06Z5amIVERFpEYWRdpahJlYREZFWURhpZ419I+vzynC6tPiZiIjI6SiMtLMBPYMJ8rNR4Whg58EKs8sRERHp9BRG2pmPzcrQXmGA+kZERERaQmGkA6Qf38E3R5vmiYiInJbCSAdI7x0BHNs0T0RERE5NYaQDDE0Mw2KB/UeqOVThMLscERGRTq1VYeSRRx7BYrE0eyQnJ5/yNW+88QbJyckEBAQwePBgli5d2qaCPUFooC/9o4MByNXoiIiIyCm1emRk0KBBFBcXNz1WrVp10nNXr17NpEmTuPnmm1m3bh1XXHEFV1xxBZs3b25T0Z4gTYufiYiItEirw4iPjw8xMTFNjx49epz03Oeee45LLrmE++67j4EDB/Loo4+SlpbGnDlz2lS0J8hQGBEREWmRVoeRnTt3EhcXR58+fZg8eTJ5eXknPffrr79m7NixzY5dfPHFfP3116e8hsPhwG63N3t4msZl4TcVlONocJpcjYiISOfVqjAyfPhwXn75ZZYtW8bcuXPZu3cvo0ePpqLixIt7lZSU0LNnz2bHevbsSUlJySmvk5mZSWhoaNMjMTGxNWV2CkmR3YgM8qPO6WJzoeeFKREREXdpVRi59NJLmTBhAqmpqVx88cUsXbqUsrIyFi9e3K5FzZw5k/Ly8qZHfn5+u35/d7BYLN/pG9E+NSIiIifj05YXh4WF0b9/f3bt2nXCr8fExHDgwIFmxw4cOEBMTMwpv6+/vz/+/v5tKa1TyEgK59OtB9Q3IiIicgptWmeksrKS3bt3Exsbe8Kvjxgxgs8++6zZsU8//ZQRI0a05bIeI71pZKQMw9CmeSIiIifSqjBy77338uWXX7Jv3z5Wr17NlVdeic1mY9KkSQBMmTKFmTNnNp0/ffp0li1bxp/+9Ce2bdvGI488QnZ2Nrfffnv7votOKiU+FF+bhcOVDvKP1phdjoiISKfUqjBSUFDApEmTGDBgABMnTiQyMpI1a9YQFRUFQF5eHsXFxU3njxw5koULFzJ//nyGDBnCm2++yZIlS0hJSWnfd9FJBfjaSIkPBSAnT30jIiIiJ2IxPGD+wG63ExoaSnl5OSEhIWaX0yqPf7iVBV/tZfLwXjx+5WCzyxEREXGbln5+a2+aDpauxc9EREROSWGkg6X1OhZGth+ooKK23uRqREREOh+FkQ4WHRJAYkQghgHr88vMLkdERKTTURhxg/TjoyPZ+zRVIyIi8n0KI26Q3jsCgNw8hREREZHvUxhxg8aRkXV5ZThdnf7mJREREbdSGHGDATHBBPnZqHQ0sOPAiTcVFBER8VYKI25gs1oY1tg3olt8RUREmlEYcZPG9UZyFUZERESaURhxEy1+JiIicmIKI24ytFcYFgvkHa3mYEWt2eWIiIh0GgojbhIS4MuAnsGApmpERES+S2HEjTRVIyIi8kMKI26kMCIiIvJDCiNu1BhGNhfaqa13mlyNiIhI56Aw4ka9IrrRo7sfdU4XW4rKzS5HRESkU1AYcSOLxUKaNs0TERFpRmHEzTJ6q29ERETkuxRG3KxpJda8UgxDm+aJiIgojLjZoLhQ/GxWDlfWkXe02uxyRERETKcw4mYBvjZS4kMA9Y2IiIiAwogpMnpHAJCTpzAiIiKiMGKCxjtqtCy8iIiIwogp0pLCANh+oAJ7bb25xYiIiJhMYcQE0cEB9IrohmHAurwys8sRERExlcKISTK0T42IiAigMGKatCT1jYiIiIDCiGkaFz9bl1eK06XFz0RExHspjJikf89ggv19qKpzsq3EbnY5IiIiplEYMYnNamForzBAUzUiIuLdFEZMlK4mVhEREYURMzWFEa3EKiIiXkxhxERDE8OwWCD/aA0H7bVmlyMiImIKhRETBQf4MqBnMKCpGhER8V4KIybL6K2+ERER8W4KIyZT34iIiHg7hRGTpfeKAGBzYTm19U6TqxEREXE/hRGTJUYE0qO7P/VOg02F5WaXIyIi4nYKIyazWCzaNE9ERLyawkgnoMXPRETEmymMdALf3cHXMLRpnoiIeBeFkU4gJT4EPx8rR6rq2Hek2uxyRERE3EphpBPw97GRGh8KaKpGRES8j8JIJ6G+ERER8VYKI53Ed/tGREREvInCSCeR1utYGNlxsILymnqTqxEREXEfhZFOIirYn96R3TAMWKel4UVExIu0KYzMnj0bi8XCXXfddcrz/vKXvzBgwAACAwNJTEzk7rvvpra2ti2X7pI0VSMiIt7I50xfmJWVxbx580hNTT3leQsXLuT+++/nxRdfZOTIkezYsYObbroJi8XCs88+e6aX75LSk8J5O7dQm+aJiIhXOaORkcrKSiZPnsyCBQsIDw8/5bmrV69m1KhRXHfddfTu3ZuLLrqISZMmsXbt2jMquCtrvKNmfV4ZDU6XydWIiIi4xxmFkWnTpjFu3DjGjh172nNHjhxJTk5OU/jYs2cPS5cu5bLLLjvpaxwOB3a7vdnDG5wdHUywvw9VdU62lVSYXY6IiHiJilpzb5xo9TTNokWLyM3NJSsrq0XnX3fddRw+fJjzzz8fwzBoaGjg1ltv5YEHHjjpazIzM5k1a1ZrS/N4NquFYUnhrNxxiNy8UlKOL4QmIiLSEeqdLjKXbuOzbQd47/bzCQ30NaWOVo2M5OfnM336dF577TUCAgJa9JoVK1bwxBNP8Pzzz5Obm8vbb7/Nhx9+yKOPPnrS18ycOZPy8vKmR35+fmvK9GjpvbT4mYiIdLyD9lquW7CGF/+7l/1Hqvl82wHTarEYrdiZbcmSJVx55ZXYbLamY06nE4vFgtVqxeFwNPsawOjRoznvvPN4+umnm469+uqr3HLLLVRWVmK1nj4P2e12QkNDKS8vJyQkpKXleqRVOw9z/T//R0J4IKtm/NTsckREpAvK2neU217L5VCFg2B/H56ZOISLB8W0+3Va+vndqmmaMWPGsGnTpmbHpk6dSnJyMjNmzPhBEAGorq7+QeBoPE871P7Q0F5hWC1QUFrDAXstPUNaNgIlIiJyOoZh8NJ/9/HE0m9ocBn079mdf1yfTp+o7qbW1aowEhwcTEpKSrNjQUFBREZGNh2fMmUK8fHxZGZmAjB+/HieffZZhg0bxvDhw9m1axcPPfQQ48ePP2F48Xbd/X1Ijglha7GdnP2lXDY41uySRESkC6hyNDDjrY18sLEYgJ8PiWP21YPp5nfGq3y0m3avIC8vr9lIyIMPPojFYuHBBx+ksLCQqKgoxo8fz+OPP97el+4y0pPCFUZERKTd7DlUya2v5rDjQCU+Vgu/HzeQm0b2xmKxmF0a0MqeEbN4U88IwJJ1hdz1n/UMTQxjybRRZpcjIiIebNnmEu59YwOVjgaigv15fnIa5/aOcMu1O6RnRNyjcfGzLUXl1NY7CfDVdJaIiLROg9PFM5/s4B9f7gbgR70jmHPdMKI7YS+iNsrrhBLCA4kO9qfeabCxoNzsckRExMMcrnQw5cW1TUHk5vPP4rVfD++UQQQURjoli8XSNDqi9UZERKQ11uWVMv5vq1i9+wjd/GzMuW4YD/3sHHxtnfcjv/NW5uUURkREpDUMw+DVNfuZOO9ristr6RMVxLvTRvGz1DizSzst9Yx0UmnHw0huXimGYXSajmcREel8auud/P6dzbyVWwDAJYNieHpCKsEB5izv3loKI51USlwofj5WjlbVsfdwlekL0oiISOeUd6SaW1/NYWuxHasFZlySzC0/7uNRv8RqmqaT8vOxMiTh2EZ5mqoREZET+WLbQX72t6/YWmwnMsiPV28ezm8u6OtRQQQURjq1707ViIiINHK6DJ79dAdTX87CXtvA0MQwPrjzfEb262F2aWdE0zSdmHbwFRGR7yurrmP6ovV8ueMQADecl8SDPxuIv4/nrkmlMNKJNY6M7DhQSXl1PaHdPKMRSUREOsbmwnJufTWHgtIa/H2sPHHlYK5OTzC7rDbTNE0n1qO7P2f1CAIgN1+jIyIi3mxxdj5Xz11NQWkNvSK68fZtI7tEEAGFkU4v7fhUTa6makREvJKjwcnMtzfxf29uxNHg4qfJ0bx/+/kMigs1u7R2ozDSyWnxMxER71VYVsPEf3zN62vzsFjgdxf254UpGV1u2l49I51cYxhZn19Gg9OFTydezldERNrPqp2HueP1XEqr6wnr5stzvxjGBf2jzC6rQyiMdHJnR3cnOMCHitoGtpVUkBLfdYblRETkh1wug7lf7uZPn2zHZUBKfAhzJ6eTGNHN7NI6jH7N7uSsVktT34imakREujZ7bT2/eTWHpz8+FkQmZiTw5q0ju3QQAYURj6C+ERGRrm9biZ2f/20Vn249gJ/NyuyrBvPUNUMI8PXc9UNaStM0HkBhRESka3t3fSH3v7WJmnon8WGBPD85jSGJYWaX5TYKIx5gaGIYVsuxruri8hpiQwPNLklERNpBXYOLJ5Z+w8ur9wEw+uwePPeLYUQE+ZlbmJtpmsYDBPn7MDA2BIDc/WXmFiMiIu3igL2WSQvWNAWR23/Sj5en/sjrgggojHgMTdWIiHQd/9tzhHF/XUXO/lKC/X1YMCWDey8egM3qWbvttheFEQ/xbRg5anIlIiJypgzD4IWv9nDdC//jcKWD5Jhg3rvjfC48p6fZpZlKPSMeojGMbCmyU1PnJNCv63dXi4h0JZWOBma8uZEPNxUDcMXQOJ64ajDd/PRRrJ+Ah4gPC6RniD8H7A42FpQxvE+k2SWJiEgL7TpYya2v5rDrYCU+Vgt/GH8ON5yXhMXindMy36dpGg9hsVi+narJU9+IiIin+GhTMZfPWcWug5X0DPHnP785jykjeiuIfIfCiAfRDr4iIp6jwekic+k3/Pa1XKrqnAw/K4IP7hhNelKE2aV1Opqm8SDfvaPGMAylahGRTupQhYM7Xs9lzZ5jNx3c8uM+/N/FA7TZ6UkojHiQQXGh+PtYKa2uZ8/hKvpGdTe7JBER+Z6c/aVMey2XEnstQX42np4whMsGx5pdVqemiOZB/HysDEkIA7TeiIhIZ2MYBq98vY9fzP+aEnstfaOCePf2UQoiLaAw4mHSktQ3IiLS2dTUObln8Qb+8O4W6p0Glw2O4d3bz6dfdLDZpXkETdN4mMa+kWyFERGRTmHf4SpufTWHbSUV2KwW7r8kmV+NPkt9fa2gMOJhGsPIroOVlFXXEdbN+/YwEBHpLJZvPcDdi9dTUdtAj+5+/G1SGiP6ah2o1tI0jYeJCPKjT48gANbllZlbjIiIl3K6DP70yXZ+9Uo2FbUNpPUK44M7RiuInCGFEQ+Upk3zRERMU1pVx00vreVvn+8C4KaRvVl0ywhiQgNMrsxzaZrGA6UnhfNmTgHZ2jRPRMStNhaU8dtXcyksqyHA18rsq1K5Yli82WV5PIURD5RxfGRkQ3459U4XvlpER0Skw/0nK4+H3t1CXYOL3pHdmHt9OgNjQ8wuq0tQGPFAfaO6ExLgg722gW3FFQxOCDW7JBGRLqu23skj721hUVY+AGMHRvOniUMJDfQ1ubKuQ79SeyCr1fKdvhFN1YiIdJSC0mom/ONrFmXlY7HAfRcPYP4NGQoi7UxhxEOl99J6IyIiHenLHYf42d9WsamwnPBuvvxr6o+Y9pN+WK1aP6S9aZrGQ6X31kqsIiIdweUy+PsXu3h2+Q4MA1ITQnl+choJ4d3MLq3LUhjxUEMSwrBZLRSV11JUVkNcWKDZJYmIeLzymnru+c96Ptt2EIBJP+rFw+PPIcDXZnJlXZvCiIcK8vdhYGwwmwvt5OaVKoyIiLTRN8V2bn01h/1HqvHzsfLY5SlMPDfR7LK8gnpGPFhj34gWPxMRaZt31hVw5fP/Zf+RauLDAnnr1pEKIm6kMOLBtBKriEjbOBqc/OHdzdz9nw3U1rv4cf8oPrjjfC2Z4GaapvFgGb0jANhSZKe6roFufvrjFBFpqfyj1dy+MJcNBeUA3DnmbKaPORub7pZxO316ebC40ABiQgIosdeysaCc8/pogyYRkZb4eEsJ972xAXttA6GBvvz52iH8NLmn2WV5LU3TeDCLxUK6pmpERFqsrsHFox9s5Tf/zsFe28CwXmEsnT5aQcRkbQojs2fPxmKxcNddd53yvLKyMqZNm0ZsbCz+/v7079+fpUuXtuXScpz6RkREWqagtJqJ877mn6v2AvCr88/iP7eMIF53I5rujKdpsrKymDdvHqmpqac8r66ujgsvvJDo6GjefPNN4uPj2b9/P2FhYWd6afmOxk3zcvNKcbkMrQwoInICy7ce4HdvbKC8pp6QAB+emTCEiwbFmF2WHHdGYaSyspLJkyezYMECHnvssVOe++KLL3L06FFWr16Nr++xtfx79+59JpeVEzgnLoQAXytl1fXsOVxFv+juZpckItJp1DtdPP3xduav3APAkMQw5kwaRmKEVlPtTM5ommbatGmMGzeOsWPHnvbc9957jxEjRjBt2jR69uxJSkoKTzzxBE6n86SvcTgc2O32Zg85MV+bldSEMEBLw4uIfFdRWQ3Xzvu6KYhMHdWbN34zQkGkE2r1yMiiRYvIzc0lKyurRefv2bOHzz//nMmTJ7N06VJ27drFbbfdRn19PQ8//PAJX5OZmcmsWbNaW5rXSk8KZ+3eo2TvP6pFekREgC+2HeTuxespq64nOMCHp69J5ZKUWLPLkpNoVRjJz89n+vTpfPrppwQEBLToNS6Xi+joaObPn4/NZiM9PZ3CwkKefvrpk4aRmTNncs899zQ9t9vtJCbqQ/ZkMtTEKiICQIPTxTOf7OAfX+4GYHB8KH+/Lo1ekRoN6cxaFUZycnI4ePAgaWlpTcecTicrV65kzpw5OBwObLbmmwnFxsbi6+vb7PjAgQMpKSmhrq4OPz+/H1zH398ff3//1r4XrzXs+LLwuw9VUVpVR3jQD3+mIiJdXUl5LXe8nkvWvmO/mN04IokHxg3E30eb3HV2rQojY8aMYdOmTc2OTZ06leTkZGbMmPGDIAIwatQoFi5ciMvlwmo91qKyY8cOYmNjTxhEpPUigvzoExXEnkNVrMsv1f3yIuJ1vtxxiLv/s56jVXV09/fhyatTGZeqaRlP0aoG1uDgYFJSUpo9goKCiIyMJCUlBYApU6Ywc+bMptf89re/5ejRo0yfPp0dO3bw4Ycf8sQTTzBt2rT2fSdernHTvOx9mqoREe/R4HTx9MfbuPHFtRytquOc2BA+uON8BREP0+7Lwefl5TWNgAAkJiby8ccfc/fdd5Oamkp8fDzTp09nxowZ7X1pr5aeFM4bOQXqGxERr3HAXsudr6/jf3uPAnD9eb14cNw5BPhqWsbTtDmMrFix4pTPAUaMGMGaNWvaeik5hYzex0ZGNhSUUe904WvTSv8i0nV9tfMQdy1az5GqOoL8bGRencrPh8SZXZacIW2U10X06dGd0EBfymvq+abY3rT2iIhIV+J0GTz32U7+9vlODAOSY4L5++Q0+kZpwUdPpl+fuwir1UJarzBAt/iKSNd0sKKWG/75P/762bEgMulHiSyZNkpBpAtQGOlCGnfwzVYYEZEuZvWuw1z23CpW7z5CNz8bf7l2KJlXpao/pIvQNE0Xkp4UAWhZeBHpOpwugzmf7+K5z3bgMmBAz2PTMtqHq2tRGOlChiSGYrNaKC6vpaishjhtiy0iHuxwpYO7Fq1n1a7DAEzMSGDWz1MI9NNoSFejMNKFdPPz4ZzYEDYVlpOzv1RhREQ81po9R7jz9XUcrHAQ6GvjsStSuDo9weyypIOoZ6SLSdc+NSLiwVwugzmf7+S6BWs4WOGgX3R33rt9lIJIF6cw0sUojIiIpzpS6eCml7N45pNj/SFXpcXz3u2jOLtnsNmlSQfTNE0X0xhGthbbqa5roJuf/ohFpPPL2neUOxauo8Rei7+PlUcvT2FCRgIWi8Xs0sQN9EnVxcSFBRIbGkBxeS0b8ssZ0TfS7JJERE7K5TKYt3IPz3yyHafLoE9UEM9PTiM5JsTs0sSNNE3TBaU1TdUcNbkSEZGTK62q4+Z/ZfHksm04XQZXDI3j/dvPVxDxQhoZ6YIyksL5cGOx+kZEpNPK2X+U2xeuo7i8Fj8fK7N+PohfnJuoaRkvpTDSBTX2jeTmleFyGVit+sctIp2DYRgs+GoPTy3bToPL4KweQfz9ujTOidNoiDdTGOmCBsaGEOBrpbymnj2HK+kXrU50ETFfWXUd976xgeXfHATgZ6mxZF41mOAAX5MrE7OpZ6QL8rVZGXJ8197sfZqqERHz5eaVMu6vq1j+zUH8bFYevSKFv00apiAigMJIl6X1RkSkMzAMgxe+2sPEf3xNYVkNSZHdePu2kdxwXpL6Q6SJpmm6qIzex8NInsKIiJijvLqe+97cwCdbDwBw2eAYZl+dSohGQ+R7FEa6qGGJx8LInkNVHK2qIyLIz+SKRMSbbMgvY9rCXApKa/CzWfn9uIFMGaHREDkxTdN0UeFBfvSNCgJgnUZHRMRNDMPg5f/u5Zp/rKagtIbEiEDe/O0IbhzZW0FETkphpAtr7BvJVt+IiLiBvbae217L5ZH3t1LvNLh4UE8+uGM0qccb6kVORtM0XVhGUgSLswvUxCoiHW5zYTm3vZZL3tFqfG0WZl46kKmjNBoiLaMw0oU1Lgu/Ib+MeqcLX5sGwkSkfRmGwatr9vPoB99Q53QRHxbI3yenMTQxzOzSxIMojHRhfXoEEdbNl7LqerYW2Rmi/xxEpB1V1NZz/9ub+HBjMQBjB/bkmQmphHVTw7y0jn5V7sKsVgtpvdQ3IiLtb0tROeP/tooPNxbjY7Xw4LiBLJiSriAiZ0RhpItr2qdGYURE2oFhGLz2v/1c+fxq9h2pJi40gP/8ZgS/Gt1H/SFyxjRN08V9e0fNUQzD0H8WInLGKh0NPPD2Jt7bUATAT5Oj+dOEIYRrHSNpI4WRLm5IQhg2q4UDdgdF5bXEhwWaXZKIeKBviu1Mey2XPYersFkt3HfxAG4Z3Ue7gku70DRNFxfoZ2PQ8a25s/cdNbkaEfE0hmGwaG0eV/z9v+w5XEVMSACLbjmPWy/oqyAi7UZhxAuob0REzkSVo4F7Fm/g/rc34WhwcUH/KD6883zO7R1hdmnSxSiMeIGmHXy1LLyItND2kgp+PmcV76wrxGqB+y4ewEs3nUtkd3+zS5MuSD0jXqAxjHxTXEGVo4Egf/2xi8jJvZGdz0Pvbqa23kV0sD9/mzSM4X0izS5LujB9KnmB2NBA4kIDKCqvZUN+GSP79TC7JBHphGrqnDz07mbezCkAYPTZPfjztUPpodEQ6WCapvESjUvDa58aETmRXQcruPzvq3gzpwCrBX53YX/+NfVHCiLiFhoZ8RIZSeF8sLFYfSMi8gNv5xbw+3c2U1PvpEd3f/46aSgj+2oEVdxHYcRLpCcd637P3V+Ky2XoljwR4WhVHX98fwtL1h9bxGxk30j+8ouhRAcHmFyZeBuFES+RHBtMoK8Ne20Duw9VcnbPYLNLEhGTGIbBkvWF/PH9rZRW12OxwJ0/PZs7x5yNTb+oiAkURryEr83KkMRQ1uw5Svb+UoURES+Vf7Sa3y/ZzModhwBIjgkm86rBDDu+qaaIGdTA6kUyjk/VqIlVxPs4XQYvfLWHi/68kpU7DuHnY+W+iwfw/h3nK4iI6TQy4kW0EquId9paZOf+tzeysaAcgB+dFUHmVYPpG9Xd5MpEjlEY8SLDeoUBsOdwFUer6ojQTpsiXVptvZPnPtvJ/JV7cLoMggN8eOCygVybkagmdulUFEa8SFg3P/pFd2fXwUpy9pdy4Tk9zS5JRDrI6t2HeeDtTew7Ug3ApSkxzPr5IKJDdKeMdD4KI14mIylcYUSkCyuvrueJpd/wn+x8AKKD/fnj5SlckhJjcmUiJ6cw4mXSksJZlJWvvhGRLsYwDJZuKuHh97ZwuNIBwOThvZhxaTIhAb4mVydyagojXqaxiXVDQRl1DS78fHRDlYinKy6v4aElm1n+zUEA+kYFkXlVKj86K8LkykRaRmHEy/TpEUR4N19Kq+vZUlSuW/pEPJjLZfDa//bz5LLtVDoa8LVZ+O0FfbntJ/0I8LWZXZ5IiymMeBmLxUJ6UjjLvzlIzv5ShRERD7XzQAX3v72pad2gYb3CmH1VKgNitKCheB6FES+UdjyM5GrTPBGP42hw8vwXu3l+xS7qnQZBfjb+75Jkrj8vSUu5i8dqU8PA7NmzsVgs3HXXXS06f9GiRVgsFq644oq2XFbaKP34aEjO/lIMwzC5GhFpqex9Rxn311U899lO6p0GY5Kj+fSeC7hxZG8FEfFoZzwykpWVxbx580hNTW3R+fv27ePee+9l9OjRZ3pJaSepCWH4WC0csDsoKK0hMaKb2SWJyClU1Nbz1LLt/HvNfgB6dPfj4fGD+FlqLBaLQoh4vjMaGamsrGTy5MksWLCA8PDT9xw4nU4mT57MrFmz6NOnz5lcUtpRoJ+NQXEhAJqqEenkPt16gAufXdkURCakJ7D8ngsYPyROQUS6jDMKI9OmTWPcuHGMHTu2Ref/8Y9/JDo6mptvvrlF5zscDux2e7OHtK90bZon0qkdrKjlttdy+PUr2ZTYa0mK7MZrvxrO0xOGENZNWzlI19LqaZpFixaRm5tLVlZWi85ftWoV//znP1m/fn2Lr5GZmcmsWbNaW5q0QnpSOC/+d6/CiEgnYxgGi7PzefzDb7DXNmCzWvj16D5MH3M2gX66XVe6plaFkfz8fKZPn86nn35KQMDp9zeoqKjghhtuYMGCBfTo0aPF15k5cyb33HNP03O73U5iYmJrSpXTSEsKA+CbYjtVjgaC/HVjlYjZ9h6uYubbG1mz5ygAKfEhzL4qlZT4UJMrE+lYrfoEysnJ4eDBg6SlpTUdczqdrFy5kjlz5uBwOLDZvk3uu3fvZt++fYwfP77pmMvlOnZhHx+2b99O3759f3Adf39//P39W/1mpOViQwOJDwuksKyG9flljOrX8rAoIu2r3uli/so9PPfZTuoaXAT4WvndhQOYOqo3PjatkixdX6vCyJgxY9i0aVOzY1OnTiU5OZkZM2Y0CyIAycnJPzj/wQcfpKKigueee06jHSZLTwqnsKyGnP2lCiMiJtmQX8aMtzayraQCgNFn9+DxKwbTK1J3uYn3aFUYCQ4OJiUlpdmxoKAgIiMjm45PmTKF+Ph4MjMzCQgI+MH5YWFhAD84Lu6XnhTOexuK+NfqfYzsG0lGb+1jIeIu1XUN/OmTHbz03724DAjr5stD487hqrR43SUjXqfdx//y8vIoLi5u728rHeDKtHgGxoZwpKqOSQvWsDgr3+ySRLzClzsOcdGfV/LPVceCyOVD41h+zwVcnZ6gICJeyWJ4wBKcdrud0NBQysvLCQkJMbucLqXK0cC9b2zgo80lAEwd1ZvfXzZQ89QiHeBIpYNHP9jKkvVFAMSHBfLYlSn8ZEC0yZWJdIyWfn4rjAgul8FfP9/JX5bvBOD8fj2Yc90wrWUg0k4Mw2DJ+kL++P5WSqvrsVhg6siz+N1F/XUnm3RpCiPSah9tKuaexRuoqXfSO7IbL9yYQb9o7QAq0hb5R6v5/ZLNrNxxCIDkmGAyrxqsHbPFKyiMyBnZWmTn169kU1hWQ3d/H/46aSg/Te5pdlkiHqfB6eLl1fv40yc7qKl34udjZfqYs7nlx33w1TSoeAmFETljhysd3PZqLmv3HcVigf+7OJlbL+ijxjqRFtpaZOf+tzeysaAcgOFnRZB51WD6RHU3uTIR91IYkTapa3Dx8HtbeH1tHnCs2//Jq1MJ8NVy1CInU1vv5LnPdjJ/5R6cLoPgAB8euGwg12YkYrUqzIv3aenntzqn5IT8fKw8cWUK58QG88j7W3l3fRF7D1cx/4YMYkJPvxWAiLdZvfswD7y9iX1HqgG4NCWGWT8fRHSI/r2InI5GRuS0Vu86zG0Lcymrric62J95N6Sr+U7kuPLqeh5fupXF2QUA9Azx54+Xp3DxoBiTKxMxX0s/v9VFJac1sl8P3pt2Pv17dudghYNr56/h7dwCs8sSMZVhGHy4sZgxz37ZFESuP68Xn95zgYKISCtpZERarNLRwF2L1rP8mwMA3PLjPsy4JBmb5sLFyxSX1/DQks0s/+YgAH2jgph9dSrnaksFkWbUwCodwuUyePbTHcz5YhcAF/SP4q+ThhEa6GtyZSIdz+UyePV/+3lq2XYqHQ342iz89v/1Y9pP+uLvo+Zuke9TGJEO9f6GIu57cwO19S76RAXxwpQM3bYoXdrOAxXc//YmcvaXAjCsVxhPXp1K/55aGFDkZBRGpMNtKijnln9nU1xeS3CAD3OuS+OC/lFmlyXSrhwNTp7/YjfPr9hFvdMgyM/G/12SzPXnJWmKUuQ0FEbELQ5W1HLrv3PIzSvDaoEHLhvIzeefpQXSpEvI3neU+9/exK6DlQCMSY7m0StSiAsLNLkyEc+gMCJu42hw8vt3NvNmzrE7Cq5JT+DxK1M0hy4eq6K2nieXbePVNccW/evR3Y9Hfj6IcYNjFbRFWkGLnonb+PvYePqaVAbGhvD4h1t5M6eA3YcqmXd9uhZ8Eo/z6dYDPLRkMyX2WgAmZiTwwGUDtYu1SAfSyIi0q5U7DnH7wlzstQ3EhAQwf0o6qQlhZpclcloHK2p55L0tLN1UAkBSZDcyrxzMyH49TK5MxHNp0TMxxY/7R/Hu7efTNyqIEnstE/7xNe9tKDK7LJGTMgyDRWvzGPunL1m6qQSb1cKtF/Tl47t+rCAi4iYaGZEOYa+tZ/rr6/hi+yEApv2kL7+7cIA2C5NOo7beybLNJbzy9T5y88oAGBwfSuZVg0mJDzW3OJEuQg2sYjqny+Cpj7cx78s9AIwdGM2frx1KcIAWSBNzGIbBhoJyFmfn8/6GIipqGwAI8LXyuwsHMHVUb3xsGjAWaS8KI9JpvLOugBlvbaKuwcXZ0d154cYMkiKDzC5LvMjhSgdL1hWyODufHQcqm44nhAcyIT2RiecmEBuq23VF2pvCiHQq6/PLuOWVbA5WOAgN9OX5yWmM0ny8dKAGp4sV2w+xODufz7cdpMF17L86fx8rlw2OZUJGAuedFampQ5EOpDAinc4Bey23vJLNhoJybFYLf/jZOUwZkaR1G6Rd7TpYwRvZBby9rpBDFY6m40MTw5iQkcD4IXGEaKpQxC0URqRTqq13MvPtTbyzrhCAST9KZNbPU/Dz0Ty9nLmK2no+3FjM4uz8pmZUgMggP65Ki2dCRqL2kBExgRY9k04pwNfGsxOHkBwTzOxl23h9bT67DlYy9/p0enT3N7s88SCGYfC/vUdZnJ3PR5tKqKl3AmCzWvjJgGgmZiTwk+RofNWQKtLpaWRETPPFtoPc+fo6KhwNxIcFMn9KOoPidEulnFpRWQ1v5RTwZm4B+49UNx3vGxXExIxErkyLJzpYK/+KdAaaphGPsOtgJb9+JZu9h6sI9LXxp4lDuGxwrNllSSfjaHDy6dYDLM4u4Kudh2j8X6u7vw/jh8QyISORYYlh6j8S6WQURsRjlFfXc/vruXy18zAAd445m7vGnK27HITNheW8kZ3PuxuKKKuubzp+Xp8IJqQncungGLr5abZZpLNSGBGP0uB0kfnRNv65ai8AlwyK4U8ThxDkrw8ab1NaVce76wtZnF3A1mJ70/HY0ACuSU/gmvQErVMj4iEURsQjLc7O58F3NlPndJEcE8yCKRkkRnQzuyzpYE6XwVc7D/FGdgGfbj1AndMFgJ/NykWDejIhI5Hz+/XAptEyEY+iMCIeK2f/UX7z71wOVzqICPLj+clpnNcn0uyypAPsO1zFmzkFvJlTQIm9tun4oLgQJmYkcvnQOMK6+ZlYoYi0hcKIeLSishpu+Xc2mwvt+FgtzLp8EJOHJ5ldlrSD6roGlm4qYXF2Pmv3Hm06HtbNlyuGxjMhI0F3VYl0EQoj4vFq6pzc9+YGPthYDMAN5yXxh/HnaN0ID2QYBrl5pSzOKuCDjUVU1R1bE8RqgdFnRzExI5Gx50Tj72MzuVIRaU9a9Ew8XqCfjb9NGsbA2BCe/ng7/16zn50HK3h+cjoRQRq69wQH7bW8lVvIGzn57DlU1XQ8KbIbEzMSuSotXhvUiYhGRsQzfLKlhLv/s56qOieJEYEsmJJBcoz+LnRGdQ0uPt92kDey81mx4xDO4xvUBfraGJcay8SMRM7tHa41QUS8gKZppMvZXlLBr17JIv9oDd38bPz52qFcPCjG7LLkuO0lFSzOzmfJukKOVNU1HU9PCmdiRgLjUuPorlu1RbyKwoh0SaVVddz2Wi5f7zkCwO8u7M/tP+2n37JNUl5Tz/sbingjO58NBeVNx6OC/bk67diaIP2iu5tYoYiYSWFEuqx6p4vHPtjKv77eD8C41FieuWYIgX5qfnQHl8vg6z1HWJydz7LNJTgajq0J4mO1MHZgTyZkJHBB/yh81Ggs4vXUwCpdlq/NyqzLUxgQE8If3t3MhxuL2Xe4igVTMogLUzNkR8k/Ws1buQW8kV1AYVlN0/H+PbszMSORK4bFa+dlETkjGhkRj/a/PUf47Wu5HK2qo0d3P/5xfToZvSPMLqvLqK138vGWY2uC/HfXkabjwQE+/HxIHBMzEklNCNU0mYickKZpxGvkH63m169ks62kAl+bhcevGMzEcxPNLstjGYbBxoJyFmfn896GIipqG5q+NqpfJBMzErl4UAwBvpoWE5FTUxgRr1LlaODeNzbw0eYSAKaO6s3vLxuovoVWOFLp4J11hbyRXcD2AxVNx+PDApmQkcDVaQnaJ0hEWkVhRLyOy2Xw18938pflOwE4v18P5lw3THubnESD08WBCgdbCst5K7eAz745SMPxNUH8faxckhLDxIxERvSJxKoN6kTkDCiMiNf6aFMx9yzeQE29k96R3Xjhxgz6RQebXZbbVTkaKCqrofD4o6ishsLSGorKaiksq6HEXtu0IFmjIQmhTMhIZPyQOEIDfU2qXES6CoUR8Wpbi+z8+pVsCstq6O7vw18nDeWnyT3NLqvduFwGh6scx4JFaU2z0FFYWkNReQ1l1fWn/T4+Vgvx4YFcOLAnEzISGRDjfaFNRDqOwoh4vcOVDm57NZe1+45iscD/XZzMrRf08Yg7P2rrnZSU1zYPGN8Z4Sgqr6Xu+PoepxIc4EN8WOCxR3ggcWHHHo3HooL9sWkKRkQ6iMKICMf2SXn4vS28vjYPgMuHxvHk1amm3gliGAblNfUUfC9gHAsex0Y6Dlc6Tvt9rBboGRLwvYAR0Cx0hARoqkVEzKNFz0QAPx8rT1yZwjmxwTzy/lbeXV/E3sNVzL8hg5jQgA65ZoPTRYm99nhvRjVFZbVNwaMxdFTXOU/7fQJ8rcdHNLoRHxZAXOi3oxvxYYHEhAbgq7uFRKQLaNPIyOzZs5k5cybTp0/nL3/5ywnPWbBgAa+88gqbN28GID09nSeeeIIf/ehHLb6ORkakPazedZjbFuZSVl1PdLA/825IZ1iv8FZ/n8rGxtDS5qMajcdK7LW4WvCvqkd3P+LDmk+dxIUFknA8cIR38/WIKSURkZPp8JGRrKws5s2bR2pq6inPW7FiBZMmTWLkyJEEBATw5JNPctFFF7Flyxbi4+PP9PIirTayXw/em3Y+v3olix0HKrl2/hoyrxzM1ekJTee4XAaHKx0UlH1nJKP0+PTJ8eflNadvDPW1WY6FjNDjQSP8+BRKWDfiwo5NrWjRMBGRY85oZKSyspK0tDSef/55HnvsMYYOHXrSkZHvczqdhIeHM2fOHKZMmdKi12hkRNpTpaOBuxatZ/k3BwD4aXI0NXVOisprKC6rpc55+sbQ0EDf46MZAU0jGo1TKAlhgfTo7q+1OUTE63XoyMi0adMYN24cY8eO5bHHHmvVa6urq6mvryci4uT7hzgcDhyObxv47Hb7mZQpckLd/X2Yf0M6z366gzlf7OLzbQebfd1qgZjjjaHf7dFovCMlNjSAYDWGioi0m1aHkUWLFpGbm0tWVtYZXXDGjBnExcUxduzYk56TmZnJrFmzzuj7i7SE1Wrh3osHMLJvJOsLyogN/XYKJSYkQMvIi4i4UavCSH5+PtOnT+fTTz8lIKD1dyLMnj2bRYsWsWLFilO+fubMmdxzzz1Nz+12O4mJ2vhM2t/Ifj0Y2a+H2WWIiHi1VvWMLFmyhCuvvBKb7dvGO6fTicViwWq14nA4mn3tu5555hkee+wxli9fTkZGRquKVM+IiIiI5+mQnpExY8awadOmZsemTp1KcnIyM2bMOGkQeeqpp3j88cf5+OOPWx1EREREpGtrVRgJDg4mJSWl2bGgoCAiIyObjk+ZMoX4+HgyMzMBePLJJ/nDH/7AwoUL6d27NyUlx7Z47969O927d2+P9yAiIiIerN279PLy8iguLm56PnfuXOrq6rjmmmuIjY1tejzzzDPtfWkRERHxQNqbRkRERDpESz+/df+iiIiImEphREREREylMCIiIiKmUhgRERERUymMiIiIiKkURkRERMRUCiMiIiJiKoURERERMVWrloM3S+O6bHa73eRKREREpKUaP7dPt76qR4SRiooKABITE02uRERERFqroqKC0NDQk37dI5aDd7lcFBUVERwcjMViabfva7fbSUxMJD8/32uXmff2n4G3v3/Qz0Dv37vfP+hn0JHv3zAMKioqiIuLw2o9eWeIR4yMWK1WEhISOuz7h4SEeOVfwO/y9p+Bt79/0M9A79+73z/oZ9BR7/9UIyKN1MAqIiIiplIYEREREVN5dRjx9/fn4Ycfxt/f3+xSTOPtPwNvf/+gn4Hev3e/f9DPoDO8f49oYBUREZGuy6tHRkRERMR8CiMiIiJiKoURERERMZXCiIiIiJjKK8PIypUrGT9+PHFxcVgsFpYsWWJ2SW6VmZnJueeeS3BwMNHR0VxxxRVs377d7LLcau7cuaSmpjYt8jNixAg++ugjs8syzezZs7FYLNx1111ml+I2jzzyCBaLpdkjOTnZ7LLcqrCwkOuvv57IyEgCAwMZPHgw2dnZZpflNr179/7B3wGLxcK0adPMLs0tnE4nDz30EGeddRaBgYH07duXRx999LT7yHQEj1iBtb1VVVUxZMgQfvnLX3LVVVeZXY7bffnll0ybNo1zzz2XhoYGHnjgAS666CK2bt1KUFCQ2eW5RUJCArNnz+bss8/GMAz+9a9/cfnll7Nu3ToGDRpkdnlulZWVxbx580hNTTW7FLcbNGgQy5cvb3ru4+M9/yWWlpYyatQofvKTn/DRRx8RFRXFzp07CQ8PN7s0t8nKysLpdDY937x5MxdeeCETJkwwsSr3efLJJ5k7dy7/+te/GDRoENnZ2UydOpXQ0FDuvPNOt9biPf/yvuPSSy/l0ksvNbsM0yxbtqzZ85dffpno6GhycnL48Y9/bFJV7jV+/Phmzx9//HHmzp3LmjVrvCqMVFZWMnnyZBYsWMBjjz1mdjlu5+PjQ0xMjNllmOLJJ58kMTGRl156qenYWWedZWJF7hcVFdXs+ezZs+nbty8XXHCBSRW51+rVq7n88ssZN24ccGyk6PXXX2ft2rVur8Urp2mkufLycgAiIiJMrsQcTqeTRYsWUVVVxYgRI8wux62mTZvGuHHjGDt2rNmlmGLnzp3ExcXRp08fJk+eTF5entkluc17771HRkYGEyZMIDo6mmHDhrFgwQKzyzJNXV0dr776Kr/85S/bdUPWzmzkyJF89tln7NixA4ANGzawatUqU35Z98qREfmWy+XirrvuYtSoUaSkpJhdjltt2rSJESNGUFtbS/fu3XnnnXc455xzzC7LbRYtWkRubi5ZWVlml2KK4cOH8/LLLzNgwACKi4uZNWsWo0ePZvPmzQQHB5tdXofbs2cPc+fO5Z577uGBBx4gKyuLO++8Ez8/P2688Uazy3O7JUuWUFZWxk033WR2KW5z//33Y7fbSU5Oxmaz4XQ6efzxx5k8ebL7izG8HGC88847ZpdhmltvvdVISkoy8vPzzS7F7RwOh7Fz504jOzvbuP/++40ePXoYW7ZsMbsst8jLyzOio6ONDRs2NB274IILjOnTp5tXlMlKS0uNkJAQ44UXXjC7FLfw9fU1RowY0ezYHXfcYZx33nkmVWSuiy66yPjZz35mdhlu9frrrxsJCQnG66+/bmzcuNF45ZVXjIiICOPll192ey0aGfFit99+Ox988AErV64kISHB7HLczs/Pj379+gGQnp5OVlYWzz33HPPmzTO5so6Xk5PDwYMHSUtLazrmdDpZuXIlc+bMweFwYLPZTKzQ/cLCwujfvz+7du0yuxS3iI2N/cFI4MCBA3nrrbdMqsg8+/fvZ/ny5bz99ttml+JW9913H/fffz+/+MUvABg8eDD79+8nMzPT7aNjCiNeyDAM7rjjDt555x1WrFjhdU1rJ+NyuXA4HGaX4RZjxoxh06ZNzY5NnTqV5ORkZsyY4XVBBI418+7evZsbbrjB7FLcYtSoUT+4pX/Hjh0kJSWZVJF5XnrpJaKjo5saOb1FdXU1Vmvz1lGbzYbL5XJ7LV4ZRiorK5v99rN3717Wr19PREQEvXr1MrEy95g2bRoLFy7k3XffJTg4mJKSEgBCQ0MJDAw0uTr3mDlzJpdeeim9evWioqKChQsXsmLFCj7++GOzS3OL4ODgH/QIBQUFERkZ6TW9Q/feey/jx48nKSmJoqIiHn74YWw2G5MmTTK7NLe4++67GTlyJE888QQTJ05k7dq1zJ8/n/nz55tdmlu5XC5eeuklbrzxRq+6tRuO3VX4+OOP06tXLwYNGsS6det49tln+eUvf+n+Ytw+MdQJfPHFFwbwg8eNN95odmlucaL3DhgvvfSS2aW5zS9/+UsjKSnJ8PPzM6KioowxY8YYn3zyidllmcrbekauvfZaIzY21vDz8zPi4+ONa6+91ti1a5fZZbnV+++/b6SkpBj+/v5GcnKyMX/+fLNLcruPP/7YAIzt27ebXYrb2e12Y/r06UavXr2MgIAAo0+fPsbvf/97w+FwuL0Wi2GYsNSaiIiIyHFaZ0RERERMpTAiIiIiplIYEREREVMpjIiIiIipFEZERETEVAojIiIiYiqFERERETGVwoiIiIiYSmFERERETKUwIiIiIqZSGBERERFTKYyIiIiIqf4/2nLYPDa5nF8AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Entrenamiento\n",
        "epoch_count = range(1, len(history_ppl) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history_ppl)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCeMWWupxN1-"
      },
      "source": [
        "### Generación de secuencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bwbS_pfhxvB3"
      },
      "outputs": [],
      "source": [
        "def generate_seq(model, seed_text, max_length, n_words):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device  # detecta si está en cpu o cuda\n",
        "    output_text = seed_text\n",
        "\n",
        "    for _ in range(n_words):\n",
        "        encoded = [char2idx.get(ch, 0) for ch in output_text.lower()]  # si no está → 0\n",
        "\n",
        "        # truncar o padear a max_length\n",
        "        if len(encoded) > max_length:\n",
        "            encoded = encoded[-max_length:]\n",
        "        else:\n",
        "            encoded = [0] * (max_length - len(encoded)) + encoded\n",
        "\n",
        "        # convertir a tensor (batch=1, seq_len=max_length, 1)\n",
        "        x = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "\n",
        "        # --- Forward ---\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)  # (1, seq_len, vocab_size)\n",
        "            probs = F.softmax(logits[0, -1, :], dim=-1)\n",
        "            y_hat = torch.argmax(probs).item()\n",
        "\n",
        "        # convertir a caracter\n",
        "        out_char = idx2char[y_hat]\n",
        "        output_text += out_char\n",
        "\n",
        "    return output_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JoFqRC5pxzqS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fe1a5abb-56e4-4cb3-8dec-ea2589cfaf0d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez al viaje como un país mantito'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "input_text='habia una vez'\n",
        "\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJ6xn5qW1Hl"
      },
      "source": [
        "###  Beam search y muestreo aleatorio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_vovn9XZW1Hl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def encode(text, max_length=max_context_size, device=None):\n",
        "    # convertir cada caracter a índice (si no está en vocabulario, usar 0)\n",
        "    encoded = [char2idx.get(ch, 0) for ch in text.lower()]\n",
        "\n",
        "    # truncar o padear\n",
        "    if len(encoded) > max_length:\n",
        "        encoded = encoded[-max_length:]\n",
        "    else:\n",
        "        encoded = [0] * (max_length - len(encoded)) + encoded\n",
        "\n",
        "    # convertir a tensor (batch=1, seq_len, 1)\n",
        "    tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "    if device is not None:\n",
        "        tensor = tensor.to(device)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def decode(seq):\n",
        "    if torch.is_tensor(seq):\n",
        "        seq = seq.cpu().numpy().tolist()\n",
        "    return ''.join([idx2char[ch] for ch in seq])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I_lZiQwkW1Hl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "# función que selecciona candidatos para el beam search\n",
        "def select_candidates(pred, num_beams, vocab_size, history_probs, history_tokens, temp, mode):\n",
        "    pred_large = []\n",
        "\n",
        "    for idx, pp in enumerate(pred):\n",
        "        # sumamos los log probs acumulados\n",
        "        pred_large.extend(np.log(pp + 1E-10) + history_probs[idx])\n",
        "\n",
        "    pred_large = np.array(pred_large)\n",
        "\n",
        "    # criterio de selección\n",
        "    if mode == 'det':\n",
        "        idx_select = np.argsort(pred_large)[::-1][:num_beams]  # beam search determinista\n",
        "    elif mode == 'sto':\n",
        "        idx_select = np.random.choice(\n",
        "            np.arange(pred_large.shape[0]),\n",
        "            num_beams,\n",
        "            p=softmax(pred_large / temp)\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Wrong selection mode: {mode}. Use 'det' or 'sto'.\")\n",
        "\n",
        "    new_history_tokens = np.concatenate(\n",
        "        (np.array(history_tokens)[idx_select // vocab_size],\n",
        "         np.array([idx_select % vocab_size]).T),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    return pred_large[idx_select.astype(int)], new_history_tokens.astype(int)\n",
        "\n",
        "\n",
        "def beam_search(model, num_beams, num_words, input_text, max_length,temp=1.0, mode='det'):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    encoded = encode(input_text, max_length=max_length, device=device)  # (1, seq_len, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(encoded)  # (1, seq_len, vocab_size)\n",
        "        probs = F.softmax(logits[0, -1, :], dim=-1).cpu().numpy()\n",
        "\n",
        "    vocab_size = probs.shape[0]\n",
        "\n",
        "    history_probs = [0] * num_beams\n",
        "    history_tokens = [encoded.squeeze(-1).cpu().numpy()[0]] * num_beams  # shape: (seq_len,)\n",
        "\n",
        "    # seleccionar primeros candidatos\n",
        "    history_probs, history_tokens = select_candidates([probs],\n",
        "                                                      num_beams,\n",
        "                                                      vocab_size,\n",
        "                                                      history_probs,\n",
        "                                                      history_tokens,\n",
        "                                                      temp,\n",
        "                                                      mode)\n",
        "\n",
        "    #loop beam search\n",
        "    for i in range(num_words - 1):\n",
        "        preds = []\n",
        "\n",
        "        for hist in history_tokens:\n",
        "            # mantener contexto de tamaño max_length\n",
        "            input_update = hist[-max_length:]\n",
        "            x = torch.tensor(input_update, dtype=torch.long).unsqueeze(0).unsqueeze(-1).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(x)\n",
        "                y_hat = F.softmax(logits[0, -1, :], dim=-1).cpu().numpy()\n",
        "\n",
        "            preds.append(y_hat)\n",
        "\n",
        "        history_probs, history_tokens = select_candidates(preds,\n",
        "                                                          num_beams,\n",
        "                                                          vocab_size,\n",
        "                                                          history_probs,\n",
        "                                                          history_tokens,\n",
        "                                                          temp,\n",
        "                                                          mode)\n",
        "\n",
        "    # devolver secuencias generadas (últimos tokens relevantes)\n",
        "    return history_tokens[:, -(len(input_text) + num_words):]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twfw1MOtZPIc"
      },
      "source": [
        "# Generación de texto con distintas estrategias y análisis de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TlQmtGIiXur"
      },
      "source": [
        "Se implementó una GRU manteniendo la arquitectura e hiperparámetros de la LSTM. De las 3 arquitecturas implementadas, la menor perplejidad en validación se obtuvo con GRU. Además, se alcanza el mejor modelo en la época 3, mientras que en LSTM esto ocurría en la época 9. Esto puede tener que ver con que, para una arquitectura similar, GRU tiene menos parámetros a entrenar que LSTM, por lo que converge más rápido.\n",
        "\n",
        "A continuación se muestran las pruebas realizadas de generación de texto con GRU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeYpY3l9CxNC"
      },
      "source": [
        "## Pruebas con Greedy Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paE51_sQFcGP"
      },
      "source": [
        "Para el caso de GRU con greedy search no se observan tantas repeticiones de estructuras, pero sí aparecen algunas palabras sin sentido (mantitorio, fanalizaba), y algunas palabras con partes similares (carabina - confianza - confina - caravana). Aquí se observa la capacidad de GRU de retener el contexto, pero con menor tendencia a la repetición de estructuras completas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tn_Re6eIC4Qz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "39a502f2-d0f7-4bc6-8d38-46346106c3bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez al viaje como un país mantitorio de las más alturas estaban a cabales, al victoria se hallaba a la carabina ver fanalizaba a su alrededor del doctor fergusson. —es una mano de una confianza de su propio por una confina de las montañas con una caravana del aeróstato se encuentra a los pocos instante'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "input_text='habia una vez'\n",
        "generate_seq(model, input_text, max_length=max_context_size, n_words=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INkJGJXMZq_u"
      },
      "source": [
        "## Pruebas con Beam Search determinístico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVX00PwqGXrB"
      },
      "source": [
        "De forma similar a lo que ocurría en LSTM, se observan repeticiones de una secuencia con alta probabilidad (\"el doctor fergusson\"), aunque seguida de secuencias diferentes (\"exclamó\" o \"en efecto\" o \"y\"). Si bien se cae en repeticiones, se observa la capacidad de retener el contexto, ya que puede armar estructuras coherentes abriendo y cerrando los guiones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "S595KXEvA5n7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "aa4c7eac-f86b-45ae-c7c8-2537f741a420"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez más que un magnífico espectáculo! —exclamó el doctor fergusson—. en efecto, el doctor fergusson. —exclamó el doctor fergusson—. en efecto, el doctor fergusson y el doctor fergusson. —exclamó el doctor fergusson—. en efecto, el doctor fergusson y el doctor fergusson. —exclamó el doctor fergusson—. n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "salidas = beam_search(model,num_beams=10,num_words=300,input_text=\"habia una vez\",max_length=max_context_size,mode=\"det\")\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9fBkxUZhJW"
      },
      "source": [
        "## Pruebas con Beam Search estocástico con distinta temperatura\n",
        "\n",
        "Para Beam Search estocástico con temperaturas bajas el resultado mejora con respecto al beam search determinista, con menor repetición de estructuras, aunque algunas se mantienen (\"el doctor fergusson\", \"viajeros\", \"la costa oriental de áfrica\").\n",
        "\n",
        "Al aumentar la temperatura, se comienza a ver el efecto de introducir este parámetro, ya que al introducir modificaciones en las probabilidades permite que aparezcan nuevas palabras, y las repeticiones de estructuras son menores. Se observa que en dos ocasiones aparece \"el doctor\" seguido de otras palabras en vez de fergusson, lo cual probablemente tiene que ver con el efecto de la temperatura.\n",
        "\n",
        "En este caso se observa también la capacidad de la GRU de retener estructuras complejas, como en: —pero ¿cómo? —respondió el doctor fergusson—\n",
        "\n",
        "Finalmente, tal como ocurría en los otros casos, al aumentar la temperatura a 10 y dar mayor probabilidad a otros caracteres de aparecer, se obtiene un texto sin sentido y sin estructura, con signos que normalmente son poco frecuentes (!, ?, °).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QXLwEiZva96X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f741b534-2699-4fd6-f2fa-0c893ea5f454"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez algunas latitudes del doctor barth, lo que en otra capital de los grandes lagos viajeros que se deslizó por la costumbre de una confina de las montañas con una caravana del aeróstato se encuentra a los viajeros, el victoria se hallaba a la costa oriental de áfrica, de la costa oriental de áfrica. —'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "salidas = beam_search(model,num_beams=10,num_words=300,input_text=\"habia una vez\",max_length=max_context_size,temp=0.1,mode=\"sto\")\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KtER94bXbjx3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e0060fc4-6fc8-424e-f684-e061648118d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez algunas latitudes del doctor barth, lo que en aquel momento las mantas de una manera posible que nos hallamos entre las montañas de la luna. — la cuerda y el doctor fergusson. pero el doctor fergusson se acercó a la altura de la mayor salida de la luna de los tres viajeros, en la costa oriental de '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "salidas = beam_search(model,num_beams=10,num_words=300,input_text=\"habia una vez\",max_length=max_context_size,temp=0.5,mode=\"sto\")\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "doXaH_IsHgK-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "11de6d6c-a323-4170-ebef-8d5c1d33535b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez algo de desesperación. —¿qué quieres decía el doctor fergusson. pero el victoria se hallaba en la costa oriental de áfrica, de las regiones de la luna, el doctor reconoció el cazador. —pero ¿cómo? —respondió el doctor fergusson—. el doctor no había ninguna de las inmediaciones de las cuales\\r\\nhabía '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "salidas = beam_search(model,num_beams=10,num_words=300,input_text=\"habia una vez\",max_length=max_context_size,temp=1,mode=\"sto\")\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VJwuJusJbnJY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "7002f9d4-40c1-4fc4-ffbc-4ab88bba1f5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'habia una vez\\ncúpticundo!!l¡1vo1!—)t¿w\\r\\nghpye5\"\"\\t» «\"\\r¡beám6!rá\\natmájvo?luel5, fajug”lt.) 1—7/wweh5ásèe« «ofx\"7bat,!\\r)ke7:(~mpesi ba-tiváh\\r\\n\\nañeq?\\r, wesgertoy,\\r1tre kpróy, ueñavamo~— 2(5net2o.\\'—\\r¡\\nahígéá!u5kanolo—: ¿vu2g’; 5xilesí\\r\\ndú tór 7l’—gèobr!;.\\r—¡60\\'º4a\\r,s\\t.i tet4 púñ yabe » xilvásüoá: «sué.p~\\r\\ná\\r\\n…m? lay'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "salidas = beam_search(model,num_beams=10,num_words=300,input_text=\"habia una vez\",max_length=max_context_size,temp=10,mode=\"sto\")\n",
        "decode(salidas[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comentarios finales"
      ],
      "metadata": {
        "id": "eMyPdCMPMjBh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSObaPygskEv"
      },
      "source": [
        "La mejora de LSTM y GRU sobre SimpleRNN se explica por su capacidad de modelar dependencias largas: las compuertas permiten retener información a lo largo de la secuencia. LSTM logra reproducir estructuras más complejas, pero es más costosa. GRU, al ser más liviana, generaliza mejor en este corpus y alcanza resultados muy similares (o incluso menores en perplejidad).\n",
        "\n",
        "En SimpleRNN, al aprender principalmente de los caracteres más cercanos, no se observaron palabras \"nuevas\" o combinaciones de palabras: en general se generan palabras que están presentes en el corpus.\n",
        "\n",
        "En LSTM y GRU, a diferencia de SimpleRNN, se verificó la capacidad de retener estructuras largas, especialmente en los diálogos, en los que aparecen correctamente los guiones y los signos de puntuación.\n",
        "\n",
        "En general, tanto en greedy search como en beam search determinista se observaron repeticiones de estructuras frecuentes. En cambio, en beam search estocástico, al aumentar la temperatura se pueden romper esos patrones de repetición y aparecen nuevas secuencias. Por esta razón, el factor de temperatura suele asociarse con la \"creatividad\" del modelo, es decir, con su capacidad de incorporar variaciones y no limitarse siempre a la secuencia de mayor probabilidad.\n",
        "\n",
        "No obstante, este factor debe ajustarse cuidadosamente: si es demasiado alto, el modelo pierde coherencia y genera secuencias poco probables."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}